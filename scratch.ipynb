{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71719535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/m/mpowell/.conda/envs/hpml_env/lib/python3.13/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "sys.path.insert(1, './FourCastNet/') # insert code repo into path\n",
    "\n",
    "\"\"\"\n",
    "*******************************\n",
    "Usage:\n",
    "Run this script by passing arguments to the command line as follows:\n",
    "\n",
    "python base_script.py --torch.compile --quantization --num-gpus XX --prediction-length XX -- ensemble_size XX --variable\n",
    "\n",
    "\n",
    "--torch.compile: Boolean, choose if running in compile mode for speedup\n",
    "--quantization: Boolean, choose if you'd like to run with quantized linear layer weights\n",
    "--num-gpus: Int, number of GPUs to use for distributed inference\n",
    "--prediction-length: Int, number of timesteps for autoregressive loop\n",
    "--variable: String, variable name you'd like to calculate. Options are:\n",
    "    variables = ['u10' (10 metre zonal wind speed m s-1),\n",
    "             'v10' (10 metre meridional wind speed m s-1),\n",
    "             't2m' (2 metre temperature K),\n",
    "             'sp' (Surface pressure Pa),\n",
    "             'msl' (Mean sea level pressure Pa),\n",
    "             't850' (temperature at the 850 hPa pressure level K),\n",
    "             'u1000' (zonal wind at 1000 mbar pressure surface m s-1),\n",
    "             'v1000' (meridional wind at 1000 mbar pressure surface m s-1),\n",
    "             'z1000' (vertical wind at 1000 mbar pressure surface m s-1),\n",
    "             'u850' (zonal wind at 850 mbar pressure surface m s-1),\n",
    "             'v850' (meridional wind at 850 mbar pressure surface m s-1),\n",
    "             'z850' (vertical wind at 850 mbar pressure surface m s-1),\n",
    "             'u500' (zonal wind at 500 mbar pressure surface m s-1),\n",
    "             'v500' (meridional wind at 500 mbar pressure surface m s-1),\n",
    "             'z500' (vertical wind at 500 mbar pressure surface m s-1),\n",
    "             't500' (temperature wind at 500 mbar pressure surface K),\n",
    "             'z50'  (geopotential height at 50 hPa),\n",
    "             'r500' (relative humidity at 500 mbar pressure surface),\n",
    "             'r850' (relative humidity at 850 mbar pressure surface),\n",
    "             'tcwv' (total column water vapor kg m-2)]\n",
    "\n",
    "*******************************\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# you may need to\n",
    "# !pip install ruamel.yaml einops timm\n",
    "# (or conda install)\n",
    "\n",
    "from utils.YParams import YParams\n",
    "from networks.afnonet import AFNONet\n",
    "\n",
    "from constants import VARIABLES\n",
    "from proj_utils import load_model, inference\n",
    "from quantize import replace_linear_with_target_and_quantize, W8A16LinearLayer, model_size\n",
    "\n",
    "PLOT_INPUTS = False # to get a sample plot\n",
    "QDTYPE = torch.int8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9c1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPILE = False\n",
    "QUANTIZE = False\n",
    "distributed = True\n",
    "prediction_length = 20\n",
    "ensemble_size = 20\n",
    "field = 'u10'\n",
    "\n",
    "base_path = \"/pscratch/sd/m/mpowell/hpml/\"\n",
    "# data and model paths\n",
    "data_path = f\"{base_path}ccai_demo/data/FCN_ERA5_data_v0/out_of_sample\"\n",
    "data_file = os.path.join(data_path, \"2018.h5\")\n",
    "model_path = f\"{base_path}ccai_demo/model_weights/FCN_weights_v0/backbone.ckpt\"\n",
    "global_means_path = f\"{base_path}ccai_demo/additional/stats_v0/global_means.npy\"\n",
    "global_stds_path = f\"{base_path}ccai_demo/additional/stats_v0/global_stds.npy\"\n",
    "time_means_path = f\"{base_path}ccai_demo/additional/stats_v0/time_means.npy\"\n",
    "land_sea_mask_path = f\"{base_path}ccai_demo/additional/stats_v0/land_sea_mask.npy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d529aa1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture used = afno\n",
      "Load time: 1.3829610908869654 seconds\n"
     ]
    }
   ],
   "source": [
    "# default\n",
    "config_file = \"./FourCastNet/config/AFNO.yaml\"\n",
    "config_name = \"afno_backbone\"\n",
    "params = YParams(config_file, config_name)\n",
    "print(\"Model architecture used = {}\".format(params[\"nettype\"]))\n",
    "\n",
    "if PLOT_INPUTS:\n",
    "    sample_data = h5py.File(data_file, 'r')['fields']\n",
    "    print('Total data shape:', sample_data.shape)\n",
    "    timestep_idx = 0\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "    for i, varname in enumerate(['u10', 't2m', 'z500', 'tcwv']):\n",
    "        cm = 'bwr' if varname == 'u10' or varname == 'z500' else 'viridis'\n",
    "        varidx = VARIABLES.index(varname)\n",
    "        ax[i//2][i%2].imshow(sample_data[timestep_idx, varidx], cmap=cm)\n",
    "        ax[i//2][i%2].set_title(varname)\n",
    "    fig.tight_layout()\n",
    "\n",
    "# import model\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# in and out channels: FourCastNet uses 20 input channels corresponding to 20 prognostic variables\n",
    "in_channels = np.array(params.in_channels)\n",
    "out_channels = np.array(params.out_channels)\n",
    "params['N_in_channels'] = len(in_channels)\n",
    "params['N_out_channels'] = len(out_channels)\n",
    "params.means = np.load(global_means_path)[0, out_channels] # for normalizing data with precomputed train stats\n",
    "params.stds = np.load(global_stds_path)[0, out_channels]\n",
    "params.time_means = np.load(time_means_path)[0, out_channels]\n",
    "\n",
    "# load the model\n",
    "if params.nettype == 'afno':\n",
    "    model = AFNONet(params).to(device)  # AFNO model\n",
    "else:\n",
    "    raise Exception(\"not implemented\")\n",
    "# load saved model weights\n",
    "model = load_model(model, params, model_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9ebe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUANTIZE:\n",
    "    param_size, buffer_size = model_size(model)\n",
    "    init_size = param_size + buffer_size\n",
    "    print(f\"Initial model size: {(init_size) / (1024 ** 2):.2f} MB, {param_size / (1024 ** 2):.2f} MB (parameters), {buffer_size /(1024 ** 2):.2f} MB (buffers)\")\n",
    "    print(QDTYPE)\n",
    "    replace_linear_with_target_and_quantize(model, W8A16LinearLayer, QDTYPE)\n",
    "    param_size, buffer_size = model_size(model)\n",
    "    final_size = param_size + buffer_size\n",
    "    print(f\"Final model size: {(final_size) / (1024 ** 2):.2f} MB, {param_size / (1024 ** 2):.2f} MB (parameters), {buffer_size /(1024 ** 2):.2f} MB (buffers)\")\n",
    "    wandb.log({\"model_size_reduction\":final_size/init_size}) \n",
    "\n",
    "if COMPILE:\n",
    "    model = torch.compile(model, backend = 'inductor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c369616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of time means = torch.Size([1, 20, 720, 1440])\n",
      "Shape of std = torch.Size([20])\n",
      "Loading inference data\n",
      "Inference data from /pscratch/sd/m/mpowell/hpml/ccai_demo/data/FCN_ERA5_data_v0/out_of_sample/2018.h5\n",
      "(20, 20, 720, 1440)\n",
      "Shape of data = (20, 20, 720, 1440)\n",
      "Running inference for variable \n"
     ]
    }
   ],
   "source": [
    "# move normalization tensors to gpu\n",
    "# load time means: represents climatology\n",
    "img_shape_x = 720\n",
    "img_shape_y = 1440\n",
    "\n",
    "# means and stds over training data\n",
    "means = params.means\n",
    "stds = params.stds\n",
    "\n",
    "# load climatological means\n",
    "time_means = params.time_means # temporal mean (for every pixel)\n",
    "m = torch.as_tensor((time_means - means)/stds)[:, 0:img_shape_x]\n",
    "m = torch.unsqueeze(m, 0)\n",
    "# these are needed to compute ACC and RMSE metrics\n",
    "m = m.to(device, dtype=torch.float)\n",
    "std = torch.as_tensor(stds[:,0,0]).to(device, dtype=torch.float)\n",
    "\n",
    "print(\"Shape of time means = {}\".format(m.shape))\n",
    "print(\"Shape of std = {}\".format(std.shape))\n",
    "\n",
    "# setup data for inference\n",
    "dt = 1 # time step (x 6 hours)\n",
    "ic = 0 # start the inference from here\n",
    "\n",
    "idx_vis = VARIABLES.index(field) # also prints out metrics for this field\n",
    "\n",
    "# get prediction length slice from the data\n",
    "print('Loading inference data')\n",
    "print('Inference data from {}'.format(data_file))\n",
    "data = h5py.File(data_file, 'r')['fields'][ic:(ic+prediction_length*dt):dt,in_channels,0:img_shape_x]\n",
    "print(data.shape)\n",
    "print(\"Shape of data = {}\".format(data.shape))\n",
    "\n",
    "\n",
    "# Announce variable name:\n",
    "print('Running inference for variable '.format(field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "385143df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from proj_utils import lat, latitude_weighting_factor, weighted_rmse_channels\n",
    "\n",
    "# A modified version of the inference script from project_utils.py\n",
    "# Adapted for distributed learning across GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afcb69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_rmse_channels(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    #takes in arrays of size [n, c, h, w]  and returns latitude-weighted rmse for each channel\n",
    "    num_lat = pred.shape[2]\n",
    "    lat_t = torch.arange(start=0, end=num_lat, device=pred.device)\n",
    "    s = torch.sum(torch.cos(3.1416/180. * lat(lat_t, num_lat)))\n",
    "    weight = torch.reshape(latitude_weighting_factor(lat_t, num_lat, s), (1, 1, -1, 1))\n",
    "    result = torch.sqrt(torch.mean(weight * (pred - target)**2., dim=(-1,-2)))\n",
    "    return result\n",
    "\n",
    "def weighted_acc_channels(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    #takes in arrays of size [n, c, h, w]  and returns latitude-weighted acc for each channel\n",
    "    num_lat = pred.shape[2]\n",
    "    lat_t = torch.arange(start=0, end=num_lat, device=pred.device)\n",
    "    s = torch.sum(torch.cos(3.1416/180. * lat(lat_t, num_lat)))\n",
    "    weight = torch.reshape(latitude_weighting_factor(lat_t, num_lat, s), (1, 1, -1, 1))\n",
    "    result = torch.sum(weight * pred * target, dim=(-1,-2)) / torch.sqrt(torch.sum(weight * pred * pred, dim=(-1,-2)) * torch.sum(weight * target *\n",
    "    target, dim=(-1,-2)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e945c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = data[np.newaxis, :, :, :]\n",
    "ensemble_init = np.tile(hold, (ensemble_size, 1, 1, 1, 1))\n",
    "\n",
    "epsilon = 1e-8\n",
    "random_values = np.random.uniform(0, 10, ensemble_size)\n",
    "\n",
    "for i in range(ensemble_size):\n",
    "    ensemble_init[i, :, :, :] *= epsilon * random_values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "052a14a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 20, 720, 1440)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e334301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "idx = idx_vis\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "# Prepare model with Accelerator\n",
    "model = accelerator.prepare(model)\n",
    "# Distribute ensemble indices\n",
    "\n",
    "print(accelerator.num_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2209ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_slice.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75ef470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data slice shape:\n",
      "torch.Size([20, 20, 720, 1440])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 45.81 MiB is free. Process 1227695 has 35.33 GiB memory in use. Including non-PyTorch memory, this process has 3.98 GiB memory in use. Of the allocated memory 3.35 GiB is allocated by PyTorch, and 133.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m     predictions[\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m] = first[\u001b[32m0\u001b[39m,idx]\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     future_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i < prediction_length - \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/hpml_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/hpml_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/u2/m/mpowell/repos/HPML_TermProject/FourCastNet/networks/afnonet.py:247\u001b[39m, in \u001b[36mAFNONet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.head(x)\n\u001b[32m    249\u001b[39m     x = rearrange(\n\u001b[32m    250\u001b[39m         x,\n\u001b[32m    251\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mb h w (p1 p2 c_out) -> b c_out (h p1) (w p2)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m         w=\u001b[38;5;28mself\u001b[39m.img_size[\u001b[32m1\u001b[39m] // \u001b[38;5;28mself\u001b[39m.patch_size[\u001b[32m1\u001b[39m],\n\u001b[32m    256\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/u2/m/mpowell/repos/HPML_TermProject/FourCastNet/networks/afnonet.py:237\u001b[39m, in \u001b[36mAFNONet.forward_features\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    235\u001b[39m B = x.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    236\u001b[39m x = \u001b[38;5;28mself\u001b[39m.patch_embed(x)\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m x = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_embed\u001b[49m\n\u001b[32m    238\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pos_drop(x)\n\u001b[32m    240\u001b[39m x = x.reshape(B, \u001b[38;5;28mself\u001b[39m.h, \u001b[38;5;28mself\u001b[39m.w, \u001b[38;5;28mself\u001b[39m.embed_dim)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 45.81 MiB is free. Process 1227695 has 35.33 GiB memory in use. Including non-PyTorch memory, this process has 3.98 GiB memory in use. Of the allocated memory 3.35 GiB is allocated by PyTorch, and 133.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "\n",
    "local_ensemble_size = (ensemble_size + accelerator.num_processes - 1) // accelerator.num_processes\n",
    "start_idx = accelerator.process_index * local_ensemble_size\n",
    "end_idx = min(start_idx + local_ensemble_size, ensemble_size)\n",
    "\n",
    "ens_idx_results = []\n",
    "for ens in range(start_idx, end_idx):\n",
    "    \n",
    "    data_slice = ensemble_init[ens] \n",
    "    data_slice = torch.tensor(data_slice, device=device, dtype=torch.float)\n",
    "    print('Data slice shape:')\n",
    "    print(data_slice.shape)\n",
    "\n",
    "    # create memory for the different stats\n",
    "    n_out_channels = params['N_out_channels']\n",
    "    acc = torch.zeros((prediction_length, n_out_channels)).to(device, dtype=torch.float)\n",
    "    rmse = torch.zeros((prediction_length, n_out_channels)).to(device, dtype=torch.float)\n",
    "\n",
    "    # to conserve GPU mem, only save one channel (can be changed if sufficient GPU mem or move to CPU)\n",
    "    targets = torch.zeros((prediction_length, 1, img_shape_x, img_shape_y)).to(device, dtype=torch.float)\n",
    "    predictions = torch.zeros((prediction_length, 1, img_shape_x, img_shape_y)).to(device, dtype=torch.float)\n",
    "\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(data_slice.shape[0]):\n",
    "            iter_start = time.perf_counter()\n",
    "            if i == 0:\n",
    "                first = data_slice[0:1]\n",
    "                future = data_slice[1:2]\n",
    "                pred = first\n",
    "                tar = first\n",
    "                # also save out predictions for visualizing channel index idx\n",
    "                targets[0,0] = first[0,idx]\n",
    "                predictions[0,0] = first[0,idx]\n",
    "                # predict\n",
    "                future_pred = model(first)\n",
    "            else:\n",
    "                if i < prediction_length - 1:\n",
    "                    future = data_slice[i+1:i+2]\n",
    "                future_pred = model(future_pred) # autoregressive step\n",
    "\n",
    "            if i < prediction_length - 1:\n",
    "                predictions[i+1,0] = future_pred[0,idx]\n",
    "                targets[i+1,0] = future[0,idx]\n",
    "\n",
    "            # compute metrics using the ground truth ERA5 data as \"true\" predictions\n",
    "            rmse[i] = weighted_rmse_channels(pred, tar) * std\n",
    "            acc[i] = weighted_acc_channels(pred-m, tar-m)\n",
    "            iter_end = time.perf_counter()\n",
    "            iter_time = iter_end - iter_start\n",
    "            \n",
    "            if accelerator.is_main_process: # Only write to wandb if we're in the main process\n",
    "                print('Predicted timestep {} of {}. {} RMS Error: {}, ACC: {}'.format(i, prediction_length, field, rmse[i,idx], acc[i,idx]))\n",
    "            \n",
    "            pred = future_pred\n",
    "            tar = future\n",
    "            total_time += iter_time\n",
    "\n",
    "    if accelerator.is_main_process: # Only write to wandb if we're in the main process\n",
    "        print(f'Total inference time: {total_time:.2f}s, Average time per step: {total_time/prediction_length:.2f}s')\n",
    "    \n",
    "    # copy to cpu for plotting and visualization\n",
    "    ens_idx_results.append({\n",
    "        \"acc\": acc.cpu().numpy,\n",
    "        \"rmse\": rmse.cpu().numpy,\n",
    "        \"total_inference_time\": total_time,\n",
    "        \"avg_time\": total_time/prediction_length,\n",
    "        \"ensemble_idx\": ens,\n",
    "    })\n",
    "\n",
    "    #Gather results across processes\n",
    "    all_results = gather_object(ens_idx_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e04b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
